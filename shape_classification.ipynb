{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shape classification\n",
    "\n",
    "The notebooks in this folder replicate the experiments as performed for [CNNs on Surfaces using Rotation-Equivariant Features](https://doi.org/10.1145/3386569.3392437).\n",
    "\n",
    "The current notebook replicates the shape classification experiments from section `5.2 Comparisons`.\n",
    "\n",
    "## Imports\n",
    "We start by importing dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File reading and progressbar\n",
    "import os.path as osp\n",
    "import progressbar\n",
    "\n",
    "# PyTorch and PyTorch Geometric dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn.inits import zeros\n",
    "\n",
    "# Harmonic Surface Networks components\n",
    "# Layers\n",
    "from nn import (HarmonicResNetBlock, HarmonicConv,\n",
    "                ParallelTransportPool, ParallelTransportUnpool,\n",
    "                ComplexNonLin)\n",
    "# Utility functions\n",
    "from utils.harmonic import magnitudes\n",
    "# Rotated MNIST dataset\n",
    "from datasets import Shrec16\n",
    "# Transforms\n",
    "from transforms import (HarmonicPrecomp, VectorHeat, MultiscaleRadiusGraph, \n",
    "                        ScaleMask, FilterNeighbours, NormalizeArea, NormalizeAxes, Subsample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "Next, we set a few parameters for our network. You can change these settings to experiment with different configurations of the network. Right now, the settings are set to the ones used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum rotation order for streams\n",
    "max_order = 1\n",
    "\n",
    "# Number of rings in the radial profile\n",
    "n_rings = 6\n",
    "\n",
    "# Number of filters per block\n",
    "nf = [16, 32]\n",
    "\n",
    "# Ratios used for pooling\n",
    "ratios=[1, 0.25]\n",
    "\n",
    "# Radius of convolution for each scale\n",
    "radii = [0.2, 0.4]\n",
    "\n",
    "# Number of datasets per batch\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "To get our dataset ready for training, we need to perform the following steps:\n",
    "1. Provide a path to load and store the dataset.\n",
    "2. Define transformations to be performed on the dataset:\n",
    "    - A transformation that computes a multi-scale radius graph and precomputes the logarithmic map.\n",
    "    - A transformation that masks the edges and vertices per scale and precomputes convolution components.\n",
    "3. Assign and load the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Provide a path to load and store the dataset.\n",
    "# Make sure that you have created a folder 'data' somewhere\n",
    "# and that you have downloaded and moved the raw datasets there\n",
    "path = osp.join('data', 'shrec')\n",
    "\n",
    "# 2. Define transformations to be performed on the dataset:\n",
    "# Transformation that computes a multi-scale radius graph and precomputes the logarithmic map.\n",
    "pre_transform = T.Compose((\n",
    "    NormalizeArea(),\n",
    "    MultiscaleRadiusGraph(ratios, radii, loop=True, flow='target_to_source'),\n",
    "    VectorHeat()\n",
    "))\n",
    "# Apply a random scale and random rotation to each shape\n",
    "transform = T.Compose((\n",
    "    T.RandomScale((0.85, 1.15)),\n",
    "    T.RandomRotate(45, axis=0),\n",
    "    T.RandomRotate(45, axis=1),\n",
    "    T.RandomRotate(45, axis=2))\n",
    ")\n",
    "\n",
    "# Transformations that masks the edges and vertices per scale and precomputes convolution components.\n",
    "scale0_transform = T.Compose((\n",
    "    ScaleMask(0),\n",
    "    FilterNeighbours(radii[0]),\n",
    "    HarmonicPrecomp(n_rings, max_order, max_r=radii[0]))\n",
    ")\n",
    "scale1_transform = T.Compose((\n",
    "    ScaleMask(1),\n",
    "    FilterNeighbours(radii[1]),\n",
    "    HarmonicPrecomp(n_rings, max_order, max_r=radii[1]))\n",
    ")\n",
    "\n",
    "# 3. Assign and load the datasets.\n",
    "test_dataset = Shrec16(path, False, pre_transform=pre_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "train_dataset = Shrec16(path, True, pre_transform=pre_transform, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "n_classes = train_dataset.num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network architecture\n",
    "Now, we create the network architecture by creating a new `nn.Module`, `Net`. We first setup each layer in the `__init__` method of the `Net` class and define the steps to perform for each batch in the `forward` method. We use the left part of the U-ResNet architecture, with half of the ResNet blocks from the following figure:\n",
    "\n",
    "<img src=\"img/resnet_architecture.png\" width=\"800px\" />\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.lin0 = nn.Linear(3, nf[0])\n",
    "\n",
    "        # ResNet Block 1\n",
    "        self.resnet_block11 = HarmonicResNetBlock(nf[0], nf[0], max_order, n_rings, prev_order=0)\n",
    "\n",
    "        # Pool\n",
    "        self.pool = ParallelTransportPool(1, scale1_transform)\n",
    "\n",
    "        # ResNet Block 2\n",
    "        self.resnet_block21 = HarmonicResNetBlock(nf[0], nf[1], max_order, n_rings)\n",
    "\n",
    "        # Final Harmonic Convolution\n",
    "        # We set offset to False, \n",
    "        # because we will only use the radial component of the features after this\n",
    "        self.conv_final = HarmonicConv(nf[1], n_classes, max_order, n_rings, offset=False)\n",
    "        \n",
    "        self.bias = nn.Parameter(torch.Tensor(n_classes))\n",
    "        zeros(self.bias)\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.pos\n",
    "        \n",
    "        # Linear transformation from input descriptors to nf[0] features\n",
    "        x = F.relu(self.lin0(x))\n",
    "        # Convert input features into complex numbers\n",
    "        x = torch.stack((x, torch.zeros_like(x)), dim=-1).unsqueeze(1)\n",
    "\n",
    "        # ResNet Block 1\n",
    "        # Select only the edges and precomputed components of the first scale\n",
    "        data_scale0 = scale0_transform(data)\n",
    "        attributes = (data_scale0.edge_index, data_scale0.precomp, data_scale0.connection)\n",
    "        x = self.resnet_block11(x, *attributes)\n",
    "\n",
    "        # Pooling\n",
    "        # Apply parallel transport pooling\n",
    "        x, data, data_pooled = self.pool(x, data)\n",
    "        \n",
    "        # ResNet Block 2\n",
    "        # Store edge_index and precomputed components of the second scale\n",
    "        attributes_pooled = (data_pooled.edge_index, data_pooled.precomp, data_pooled.connection)\n",
    "        x = self.resnet_block21(x, *attributes_pooled)\n",
    "\n",
    "        x = self.conv_final(x, *attributes_pooled)\n",
    "        \n",
    "        # Take radial component from features and sum streams\n",
    "        x = magnitudes(x, keepdim=False)\n",
    "        x = x.sum(dim=1)\n",
    "\n",
    "        # Global mean pool\n",
    "        x = torch.mean(x, dim=0, keepdim=True)\n",
    "        x = x + self.bias\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing\n",
    "\n",
    "Phew, we're through the hard part. Now, let's get to training. First, move the network to the GPU and setup an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to train on a GPU. It'll take a long time on a CPU\n",
    "device = torch.device('cuda')\n",
    "# Move the network to the GPU\n",
    "model = Net().to(device)\n",
    "# Set up the ADAM optimizer with learning rate of 0.0076 (as used in H-Nets)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define a training and test function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    # Set model to 'train' mode\n",
    "    model.train()\n",
    "\n",
    "    if epoch > 20:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.001\n",
    "\n",
    "    for data in train_loader:\n",
    "        # Move training data to the GPU and optimize parameters\n",
    "        optimizer.zero_grad()\n",
    "        F.nll_loss(model(data.to(device)), data.y).backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "def test():\n",
    "    # Set model to 'evaluation' mode\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total_num = 0\n",
    "    for i, data in enumerate(test_loader):\n",
    "        pred = model(data.to(device)).max(1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "        total_num += 1\n",
    "    return correct / total_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train for 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1587428266983/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Test: 0.0500\n",
      "Epoch 1 - Test: 0.1833\n",
      "Epoch 2 - Test: 0.3000\n",
      "Epoch 3 - Test: 0.4583\n",
      "Epoch 4 - Test: 0.6417\n",
      "Epoch 5 - Test: 0.7417\n",
      "Epoch 6 - Test: 0.8250\n",
      "Epoch 7 - Test: 0.7667\n",
      "Epoch 8 - Test: 0.8750\n",
      "Epoch 9 - Test: 0.8333\n",
      "Epoch 10 - Test: 0.8083\n",
      "Epoch 11 - Test: 0.9167\n",
      "Epoch 12 - Test: 0.9333\n",
      "Epoch 13 - Test: 0.9250\n",
      "Epoch 14 - Test: 0.8083\n",
      "Epoch 15 - Test: 0.9417\n",
      "Epoch 16 - Test: 0.9583\n",
      "Epoch 17 - Test: 0.9667\n",
      "Epoch 18 - Test: 0.9667\n",
      "Epoch 19 - Test: 0.9667\n",
      "Epoch 20 - Test: 0.9750\n",
      "Epoch 21 - Test: 0.9750\n",
      "Epoch 22 - Test: 0.9750\n",
      "Epoch 23 - Test: 0.9750\n",
      "Epoch 24 - Test: 0.9750\n",
      "Epoch 25 - Test: 0.9750\n",
      "Epoch 26 - Test: 0.9750\n",
      "Epoch 27 - Test: 0.9750\n",
      "Epoch 28 - Test: 0.9750\n",
      "Epoch 29 - Test: 0.9750\n",
      "Epoch 30 - Test: 0.9750\n",
      "Epoch 31 - Test: 0.9750\n",
      "Epoch 32 - Test: 0.9750\n",
      "Epoch 33 - Test: 0.9750\n",
      "Epoch 34 - Test: 0.9750\n",
      "Epoch 35 - Test: 0.9750\n",
      "Epoch 36 - Test: 0.9750\n",
      "Epoch 37 - Test: 0.9750\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3d5a6e900d8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Start training...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch {} - Test: {:06.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-53475a8478ff>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Move training data to the GPU and optimize parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Start training...')\n",
    "for epoch in range(50):\n",
    "    train(epoch)\n",
    "    test_acc = test()\n",
    "    print(\"Epoch {} - Test: {:06.4f}\".format(epoch, test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
